{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5324d93f-899e-480d-b7fd-0a7dcc6483b8",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "\n",
    "# -------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------\n",
    "RUTA_BARBADOS = r'C:\\Users\\dreynosoh\\Downloads\\Prueba Publicaciones.xlsx'\n",
    "RUTA_CATALOGO = r'C:\\Users\\dreynosoh\\Downloads\\Catalogo Truper agosto 25_wcm.xlsx'\n",
    "SALIDA_FINAL = r'C:\\Users\\dreynosoh\\Downloads\\publicaciones_eibel2.0.xlsx'\n",
    "\n",
    "BLOQUE = 10000  # Tama√±o del bloque (ajustar seg√∫n RAM disponible)\n",
    "\n",
    "# -------------------------------------\n",
    "# DESCARGAR STOPWORDS\n",
    "# -------------------------------------\n",
    "nltk.download('stopwords')\n",
    "stopwords_es = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# -------------------------------------\n",
    "# FUNCIONES DE PREPROCESAMIENTO\n",
    "# -------------------------------------\n",
    "def normalizar_texto(texto):\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'[^a-z0-9\\s/\\-]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def tokenize(texto):\n",
    "    tokens = texto.split()\n",
    "    unidades = {'hp', 'kg', 'ml'}\n",
    "    return {word for word in tokens if word not in stopwords_es or word in unidades}\n",
    "\n",
    "def stem_tokens(text):\n",
    "    stemmer = SpanishStemmer()\n",
    "    return {stemmer.stem(t) for t in text.split()}\n",
    "\n",
    "# -------------------------------------\n",
    "# CARGA DE DATOS\n",
    "# -------------------------------------\n",
    "barbados = pd.read_excel(RUTA_BARBADOS)\n",
    "catalogo = pd.read_excel(RUTA_CATALOGO)\n",
    "\n",
    "# -------------------------------------\n",
    "# NORMALIZAR TEXTOS\n",
    "# -------------------------------------\n",
    "barbados['Titulo_Publicacion_normalizado'] = barbados['Titulo_Publicacion'].apply(normalizar_texto).astype(str)\n",
    "catalogo['descripcion_normalizada'] = catalogo['descripci√≥n'].apply(normalizar_texto).astype(str)\n",
    "catalogo['clave'] = catalogo['clave'].fillna(\"\").astype(str)\n",
    "catalogo['Codigo'] = catalogo['Codigo'].fillna(\"\").astype(str)\n",
    "\n",
    "# -------------------------------------\n",
    "# TF-IDF VECTORIZE EL CATALOGO\n",
    "# -------------------------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_catalog = vectorizer.fit_transform(catalogo['descripcion_normalizada'])\n",
    "\n",
    "# Convertimos catalogo a lista de diccionarios\n",
    "catalog_list = catalogo[['descripcion_normalizada', 'clave', 'Codigo']].to_dict(orient='records')\n",
    "\n",
    "# -------------------------------------\n",
    "# INICIALIZAR RESULTADOS\n",
    "# -------------------------------------\n",
    "resultados = []\n",
    "\n",
    "# -------------------------------------\n",
    "# PROCESAMIENTO EN BLOQUES CON TIEMPOS\n",
    "# -------------------------------------\n",
    "n_total = len(barbados)\n",
    "n_bloques = (n_total // BLOQUE) + 1\n",
    "\n",
    "print(f\"üîÑ Procesando {n_total:,} filas en {n_bloques} bloques...\\\\n\")\n",
    "start_time_total = time.time()\n",
    "\n",
    "for i in tqdm(range(n_bloques), desc=\"üß† Procesando bloques\"):\n",
    "    bloque_start_time = time.time()\n",
    "\n",
    "    ini = i * BLOQUE\n",
    "    fin = min((i + 1) * BLOQUE, n_total)\n",
    "    bloque_df = barbados.iloc[ini:fin].copy()\n",
    "\n",
    "    textos_bloque = bloque_df['Titulo_Publicacion_normalizado'].tolist()\n",
    "    tfidf_bloque = vectorizer.transform(textos_bloque)\n",
    "    similarities = cosine_similarity(tfidf_bloque, tfidf_catalog)\n",
    "\n",
    "    for j, row in enumerate(bloque_df.itertuples(index=False)):\n",
    "        texto_norm = row.Titulo_Publicacion_normalizado\n",
    "        tokens = tokenize(texto_norm)\n",
    "        texto_sin_slash = texto_norm.replace(\"/\", \"\")\n",
    "    \n",
    "        match_found = False\n",
    "        palabra_match = None\n",
    "        clave_match = None\n",
    "        codigo_match = None\n",
    "        tipo_match = None\n",
    "        palabras_comunes = []\n",
    "        best_score = 0.0\n",
    "    \n",
    "        # --------------------------------\n",
    "        # 1Ô∏è‚É£ MATCH EXACTO: C√ìDIGO (solo si tiene 5 o 6 d√≠gitos)\n",
    "        # --------------------------------\n",
    "        for word in texto_sin_slash.split():\n",
    "            if word.isdigit() and 5 <= len(word) <= 6:\n",
    "                for item in catalog_list:\n",
    "                    if word == item[\"Codigo\"]:\n",
    "                        codigo_match = item[\"Codigo\"]\n",
    "                        clave_match = item[\"clave\"]\n",
    "                        tipo_match = \"codigo\"\n",
    "                        palabra_match = word\n",
    "                        match_found = True\n",
    "                        break\n",
    "            if match_found:\n",
    "                break\n",
    "    \n",
    "        # --------------------------------\n",
    "        # 2Ô∏è‚É£ MATCH EXACTO: CLAVE (si no hubo match por c√≥digo)\n",
    "        # --------------------------------\n",
    "        if not match_found:\n",
    "            for word in texto_sin_slash.split():\n",
    "                for item in catalog_list:\n",
    "                    if word.upper() == item[\"clave\"].replace(\"/\", \"\").upper():\n",
    "                        clave_match = item[\"clave\"]\n",
    "                        codigo_match = item[\"Codigo\"]\n",
    "                        tipo_match = \"clave\"\n",
    "                        palabra_match = word\n",
    "                        match_found = True\n",
    "                        break\n",
    "                if match_found:\n",
    "                    break\n",
    "    \n",
    "        # --------------------------------\n",
    "        # 3Ô∏è‚É£ MATCH POR COSINE SIMILARITY (si no hubo match anterior)\n",
    "        # --------------------------------\n",
    "        if not match_found:\n",
    "            sims = similarities[j]\n",
    "            best_idx = np.argmax(sims)\n",
    "            best_score = sims[best_idx]\n",
    "            best_item = catalog_list[best_idx]\n",
    "        \n",
    "            clave_match = best_item[\"clave\"]\n",
    "            codigo_match = best_item[\"Codigo\"]\n",
    "            tipo_match = \"cosine\"\n",
    "        \n",
    "            # Tokenizaci√≥n con stemming para detectar m√°s coincidencias\n",
    "            tokens_stem = stem_tokens(texto_norm)\n",
    "            descripcion_stem = stem_tokens(best_item[\"descripcion_normalizada\"])\n",
    "        \n",
    "            # Buscar coincidencias flexibles (exactas o parciales)\n",
    "            palabras_comunes = []\n",
    "            for w1 in tokens_stem:\n",
    "                for w2 in descripcion_stem:\n",
    "                    # Coincidencia directa o substring parcial\n",
    "                    if w1 == w2 or w1 in w2 or w2 in w1:\n",
    "                        palabras_comunes.append(w1)\n",
    "        \n",
    "            palabras_comunes = list(set(palabras_comunes))  # eliminar duplicados\n",
    "    \n",
    "        # --------------------------------\n",
    "        # Guardar resultado\n",
    "        # --------------------------------\n",
    "        resultados.append({\n",
    "            \"Titulo_Publicacion\": row.Titulo_Publicacion,\n",
    "            \"Titulo_Publicacion_normalizado\": texto_norm,\n",
    "            \"clave_asignada\": clave_match,\n",
    "            \"codigo_asignado\": codigo_match,\n",
    "            \"tipo_match\": tipo_match,\n",
    "            \"score_cosine\": round(best_score, 4) if not match_found else 1.0,\n",
    "            \"palabra_match\": palabra_match,\n",
    "            \"palabras_comunes\": \", \".join(palabras_comunes)\n",
    "        })\n",
    "\n",
    "    # üïí TIEMPO POR BLOQUE\n",
    "    bloque_duracion = time.time() - bloque_start_time\n",
    "    tiempo_total_actual = time.time() - start_time_total\n",
    "    tiempo_promedio = tiempo_total_actual / (i + 1)\n",
    "    bloques_restantes = n_bloques - (i + 1)\n",
    "    estimado_restante = bloques_restantes * tiempo_promedio\n",
    "\n",
    "    print(f\"\\\\nüïí Bloque {i+1}/{n_bloques} procesado en {bloque_duracion:.1f}s\")\n",
    "    print(f\"‚è≥ Tiempo total: {tiempo_total_actual/60:.1f} min\")\n",
    "    print(f\"üîÆ Estimado restante: {estimado_restante/60:.1f} min\\\\n\")\n",
    "\n",
    "    # Limpiar memoria\n",
    "    del tfidf_bloque, similarities\n",
    "    gc.collect()\n",
    "\n",
    "# -------------------------------------\n",
    "# GUARDAR RESULTADOS\n",
    "# -------------------------------------\n",
    "final_df = pd.DataFrame(resultados)\n",
    "final_df.to_excel(SALIDA_FINAL, index=False)\n",
    "tiempo_total_final = time.time() - start_time_total\n",
    "\n",
    "print(f\"\\\\n‚úÖ Proceso COMPLETADO en {tiempo_total_final/60:.1f} minutos.\")\n",
    "print(f\"üìÅ Archivo guardado en: {SALIDA_FINAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44027ec0-0657-480b-afbc-410ed75d4b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dreynosoh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Procesando 14,571 filas en 2 bloques...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Procesando bloques:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [04:23<04:23, 263.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïí Bloque 1/2 procesado en 262.6s\n",
      "‚è≥ Tiempo total: 4.4 min\n",
      "üîÆ Estimado restante: 4.4 min\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Procesando bloques: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:53<00:00, 206.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïí Bloque 2/2 procesado en 149.4s\n",
      "‚è≥ Tiempo total: 6.9 min\n",
      "üîÆ Estimado restante: 0.0 min\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Proceso COMPLETADO en 7.0 minutos.\n",
      "üìÅ Archivo guardado en: C:\\Users\\dreynosoh\\Downloads\\publicaciones_eibel2.0.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "\n",
    "# -------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------\n",
    "RUTA_BARBADOS = r'C:\\Users\\dreynosoh\\Downloads\\Prueba Publicaciones.xlsx'\n",
    "RUTA_CATALOGO = r'C:\\Users\\dreynosoh\\Downloads\\Catalogo Truper agosto 25_wcm.xlsx'\n",
    "SALIDA_FINAL = r'C:\\Users\\dreynosoh\\Downloads\\publicaciones_eibel2.0.xlsx'\n",
    "\n",
    "BLOQUE = 10000  # Tama√±o del bloque (ajustar seg√∫n RAM disponible)\n",
    "\n",
    "# -------------------------------------\n",
    "# DESCARGAR STOPWORDS\n",
    "# -------------------------------------\n",
    "nltk.download('stopwords')\n",
    "stopwords_es = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# -------------------------------------\n",
    "# FUNCIONES DE PREPROCESAMIENTO\n",
    "# -------------------------------------\n",
    "def normalizar_texto(texto):\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'[^a-z0-9\\s/\\-]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def tokenize(texto):\n",
    "    tokens = texto.split()\n",
    "    unidades = {'hp', 'kg', 'ml'}\n",
    "    return {word for word in tokens if word not in stopwords_es or word in unidades}\n",
    "\n",
    "def stem_tokens(text):\n",
    "    stemmer = SpanishStemmer()\n",
    "    return {stemmer.stem(t) for t in text.split()}\n",
    "\n",
    "# -------------------------------------\n",
    "# CARGA DE DATOS\n",
    "# -------------------------------------\n",
    "barbados = pd.read_excel(RUTA_BARBADOS)\n",
    "catalogo = pd.read_excel(RUTA_CATALOGO)\n",
    "\n",
    "# -------------------------------------\n",
    "# NORMALIZAR TEXTOS\n",
    "# -------------------------------------\n",
    "barbados['Titulo_Publicacion_normalizado'] = barbados['Titulo_Publicacion'].apply(normalizar_texto).astype(str)\n",
    "catalogo['descripcion_normalizada'] = catalogo['descripci√≥n'].apply(normalizar_texto).astype(str)\n",
    "catalogo['clave'] = catalogo['clave'].fillna(\"\").astype(str)\n",
    "catalogo['Codigo'] = catalogo['Codigo'].fillna(\"\").astype(str)\n",
    "\n",
    "# -------------------------------------\n",
    "# TF-IDF VECTORIZE EL CATALOGO\n",
    "# -------------------------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_catalog = vectorizer.fit_transform(catalogo['descripcion_normalizada'])\n",
    "\n",
    "# Convertimos catalogo a lista de diccionarios\n",
    "catalog_list = catalogo[['descripcion_normalizada', 'clave', 'Codigo']].to_dict(orient='records')\n",
    "\n",
    "# -------------------------------------\n",
    "# INICIALIZAR RESULTADOS\n",
    "# -------------------------------------\n",
    "resultados = []\n",
    "\n",
    "# -------------------------------------\n",
    "# PROCESAMIENTO EN BLOQUES CON TIEMPOS\n",
    "# -------------------------------------\n",
    "n_total = len(barbados)\n",
    "n_bloques = (n_total // BLOQUE) + 1\n",
    "\n",
    "print(f\"üîÑ Procesando {n_total:,} filas en {n_bloques} bloques...\\n\")\n",
    "start_time_total = time.time()\n",
    "\n",
    "for i in tqdm(range(n_bloques), desc=\"üß† Procesando bloques\"):\n",
    "    bloque_start_time = time.time()\n",
    "\n",
    "    ini = i * BLOQUE\n",
    "    fin = min((i + 1) * BLOQUE, n_total)\n",
    "    bloque_df = barbados.iloc[ini:fin].copy()\n",
    "\n",
    "    textos_bloque = bloque_df['Titulo_Publicacion_normalizado'].tolist()\n",
    "    tfidf_bloque = vectorizer.transform(textos_bloque)\n",
    "    similarities = cosine_similarity(tfidf_bloque, tfidf_catalog)\n",
    "\n",
    "    for j, row in enumerate(bloque_df.itertuples(index=False)):\n",
    "        texto_norm = row.Titulo_Publicacion_normalizado\n",
    "        tokens = tokenize(texto_norm)\n",
    "        texto_sin_slash = texto_norm.replace(\"/\", \"\")\n",
    "    \n",
    "        match_found = False\n",
    "        palabra_match = None\n",
    "        clave_match = None\n",
    "        codigo_match = None\n",
    "        tipo_match = None\n",
    "        palabras_comunes = []\n",
    "        best_score = 0.0\n",
    "    \n",
    "        # --------------------------------\n",
    "        # 1Ô∏è‚É£ MATCH EXACTO: C√ìDIGO (solo si tiene 5 o 6 d√≠gitos)\n",
    "        # --------------------------------\n",
    "        for word in texto_sin_slash.split():\n",
    "            if word.isdigit() and 5 <= len(word) <= 6:\n",
    "                for item in catalog_list:\n",
    "                    if word == item[\"Codigo\"]:\n",
    "                        codigo_match = item[\"Codigo\"]\n",
    "                        clave_match = item[\"clave\"]\n",
    "                        tipo_match = \"codigo\"\n",
    "                        palabra_match = word\n",
    "                        match_found = True\n",
    "                        break\n",
    "            if match_found:\n",
    "                break\n",
    "    \n",
    "        # --------------------------------\n",
    "        # 2Ô∏è‚É£ MATCH EXACTO: CLAVE (si no hubo match por c√≥digo)\n",
    "        # --------------------------------\n",
    "        if not match_found:\n",
    "            for word in texto_sin_slash.split():\n",
    "                # ‚úÖ SOLO CONSIDERAR PALABRAS CON M√ÅS DE 2 CARACTERES\n",
    "                if len(word) > 2:\n",
    "                    for item in catalog_list:\n",
    "                        if word.upper() == item[\"clave\"].replace(\"/\", \"\").upper():\n",
    "                            clave_match = item[\"clave\"]\n",
    "                            codigo_match = item[\"Codigo\"]\n",
    "                            tipo_match = \"clave\"\n",
    "                            palabra_match = word\n",
    "                            match_found = True\n",
    "                            break\n",
    "                    if match_found:\n",
    "                        break\n",
    "    \n",
    "        # --------------------------------\n",
    "        # 3Ô∏è‚É£ MATCH POR COSINE SIMILARITY (si no hubo match anterior)\n",
    "        # --------------------------------\n",
    "        if not match_found:\n",
    "            sims = similarities[j]\n",
    "            best_idx = np.argmax(sims)\n",
    "            best_score = sims[best_idx]\n",
    "            best_item = catalog_list[best_idx]\n",
    "        \n",
    "            clave_match = best_item[\"clave\"]\n",
    "            codigo_match = best_item[\"Codigo\"]\n",
    "            tipo_match = \"cosine\"\n",
    "        \n",
    "            # Tokenizaci√≥n con stemming para detectar m√°s coincidencias\n",
    "            tokens_stem = stem_tokens(texto_norm)\n",
    "            descripcion_stem = stem_tokens(best_item[\"descripcion_normalizada\"])\n",
    "        \n",
    "            # Buscar coincidencias flexibles (exactas o parciales)\n",
    "            palabras_comunes = []\n",
    "            for w1 in tokens_stem:\n",
    "                for w2 in descripcion_stem:\n",
    "                    # Coincidencia directa o substring parcial\n",
    "                    if w1 == w2 or w1 in w2 or w2 in w1:\n",
    "                        palabras_comunes.append(w1)\n",
    "        \n",
    "            palabras_comunes = list(set(palabras_comunes))  # eliminar duplicados\n",
    "    \n",
    "        # --------------------------------\n",
    "        # Guardar resultado\n",
    "        # --------------------------------\n",
    "        resultados.append({\n",
    "            \"Titulo_Publicacion\": row.Titulo_Publicacion,\n",
    "            \"Titulo_Publicacion_normalizado\": texto_norm,\n",
    "            \"clave_asignada\": clave_match,\n",
    "            \"codigo_asignado\": codigo_match,\n",
    "            \"tipo_match\": tipo_match,\n",
    "            \"score_cosine\": round(best_score, 4) if not match_found else 1.0,\n",
    "            \"palabra_match\": palabra_match,\n",
    "            \"palabras_comunes\": \", \".join(palabras_comunes)\n",
    "        })\n",
    "\n",
    "    # üïí TIEMPO POR BLOQUE\n",
    "    bloque_duracion = time.time() - bloque_start_time\n",
    "    tiempo_total_actual = time.time() - start_time_total\n",
    "    tiempo_promedio = tiempo_total_actual / (i + 1)\n",
    "    bloques_restantes = n_bloques - (i + 1)\n",
    "    estimado_restante = bloques_restantes * tiempo_promedio\n",
    "\n",
    "    print(f\"\\nüïí Bloque {i+1}/{n_bloques} procesado en {bloque_duracion:.1f}s\")\n",
    "    print(f\"‚è≥ Tiempo total: {tiempo_total_actual/60:.1f} min\")\n",
    "    print(f\"üîÆ Estimado restante: {estimado_restante/60:.1f} min\\n\")\n",
    "\n",
    "    # Limpiar memoria\n",
    "    del tfidf_bloque, similarities\n",
    "    gc.collect()\n",
    "\n",
    "# -------------------------------------\n",
    "# GUARDAR RESULTADOS\n",
    "# -------------------------------------\n",
    "final_df = pd.DataFrame(resultados)\n",
    "final_df.to_excel(SALIDA_FINAL, index=False)\n",
    "tiempo_total_final = time.time() - start_time_total\n",
    "\n",
    "print(f\"\\n‚úÖ Proceso COMPLETADO en {tiempo_total_final/60:.1f} minutos.\")\n",
    "print(f\"üìÅ Archivo guardado en: {SALIDA_FINAL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
